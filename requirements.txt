Flask==2.2.2
imageio==2.25.0
keras==2.11.0
numpy==1.23.5
Pillow==9.4.0
python_resize_image==1.1.20
requests==2.28.1
tensorflow==2.11.0

'id', 'dhost', 'user', 'iup', 'date_alerte', 'dateFinContrat',
       'dlpdata', 'dlpmeta', 'raw_metier', 'dlpfilename', 'urlc', 'user_agent',
       'status', 'feature_contrat', 'feature_classification',
       'feature_department', 'feature_position_title', 'feature_download',
       'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3',
       'feature_downloadc4', 'feature_upload', 'feature_uploadc1',
       'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer

# Chargement des données
data = pd.read_csv("votre_fichier.csv")

# Feature Engineering

# Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

# Encodage des variables catégorielles
data = pd.get_dummies(data, columns=['feature_department', 'feature_position_title', 'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3', 'feature_downloadc4', 'feature_uploadc1', 'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'])

# Traitement des colonnes de texte supplémentaires
text_columns = ['iup', 'dlpdata', 'dlpmeta', 'raw_metier']
for column in text_columns:
    tfidf = TfidfVectorizer()
    tfidf_features = tfidf.fit_transform(data[column].astype(str))
    tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[column + '_' + c for c in tfidf.get_feature_names()])
    data = pd.concat([data, tfidf_df], axis=1)

 Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

import pandas as pd

# Création d'un DataFrame avec les données d'origine
data = pd.DataFrame({'raw_metier': ['NC', 'BPCE', 'MIROVA', 'NATIXIS BANK JSC', 'BPCE-IT', 'MIROVA UK', 'NATIXIS', 'USA', 'NFA',
                                   'NATIXIS INVESTMENT MANAGERS INTERNATIONAL', 'Ostrum', 'NATIXIS IM UK LIMITED',
                                   'NATIXIS LONDON BRANCH', 'NATIXIS NORTH AMERICA LLC', 'BPCE INFOGERANCE ET TECHNOLOGIES',
                                   'NATIXIS INVESTMENT MANAGERS', 'MIROVA US LLC', 'NATIXIS INTEREPARGNE', 'BPCE LEASE',
                                   'BIMPLI', 'NIE', 'NATIXIS IM, SUCURSAL EN ESPAÑA', 'NATIXIS - SUCURSAL EM PORTUGAL',
                                   'HOLDING PARIS', 'CIB_London', 'NLE', 'COMPAGNIE EUROPEENNE DE GARANTIES ET CAUTIONS SA',
                                   'BPCE FINANCEMENT', 'SEVENTURE PARTNERS', 'THEMATICS ASSET MANAGEMENT']})

# Regroupement des "raw metiers"
regroupements = {
    'MIROVA UK': 'MIROVA',
    'BPCE-IT': 'BPCE',
    'BPCE INFOGERANCE ET TECHNOLOGIES': 'BPCE',
    'BPCE LEASE': 'BPCE',
    'BPCE FINANCEMENT': 'BPCE',
    'NATIXIS BANK JSC': 'NATIXIS',
    'NATIXIS IM UK LIMITED': 'NATIXIS',
    'NATIXIS LONDON BRANCH': 'NATIXIS',
    'NATIXIS NORTH AMERICA LLC': 'NATIXIS',
    'NATIXIS INVESTMENT MANAGERS INTERNATIONAL': 'NATIXIS',
    'NATIXIS IM, SUCURSAL EN ESPAÑA': 'NATIXIS',
    'NATIXIS - SUCURSAL EM PORTUGAL': 'NATIXIS',
    'CIB_London': 'NATIXIS'
}

data['raw_metier'] = data['raw_metier'].apply(lambda x: regroupements.get(x, x))

# Regrouper les catégories restantes sous la catégorie "Other"
categories_autres = data['raw_metier'].unique()
categories_conservees = ['MIROVA', 'BPCE', 'NATIXIS']

data['raw_metier'] = data['raw_metier'].apply(lambda x: x if x in categories_conservees else 'Other')

# Répartition des "raw metiers" après les regroupements
repartition = data['raw_metier'].value_counts().reset_index()
repartition.columns = ['raw_metier', 'count']

print(repartition)

"2022 - Frise enseignes chÃ¨que Bimpli CADO.pdf AE_BE-Lot1 - 20230315094857 - Signature 1.pdf AE_TR-Lot 2 - 20230315094909 - Signature 1.pdf Annexes au cadre de rÃ©ponse.zip Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BE BPU lot 1.xlsx BE DQE lot 1.xlsx CRT LOT 1 CADO.pdf CRT LOT 2 10 02.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf RÃ©fÃ©rences TITRES CADEAUX.pdf TR BPU lot 2.xlsx TR DQE lot 2.xlsx",
       "AE Titres restaurant.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BIMPLI_Annexe - Bons Plans.pdf BIMPLI_Annexe_Liste-nationale-des-affilies-titres-restaurants.xlsx BIMPLI_Annexe_Ou commander en ligne avec sa carte resto.pdf BIMPLI_Annexe_Outils de commande Bimpli.pdf BIMPLI_Annexe_PrÃ©sentation Guest Club.pdf BIMPLI_Annexe_PrÃ©sentation dÃ©taillÃ©e de l'application Bimpli.pdf BIMPLI_Kit com nouveau client carte.zip BIMPLI_PrÃ©sentation des espaces personnel.pdf BPU-DQE.xlsx CCP - Annexe - RGPD.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MÃ©moire Technique_BIMPLI.pdf",
       "AE23S0002.pdf Annexe - Bons Plans Bimpli.pdf Annexe - Nos outils de commande.pdf Annexe - Ou commander en ligne.pdf Annexe - Plan_d'accompagnement_bÃ©nÃ©ficiaires.pdf Annexe - PrÃ©sentation Des Espaces personnels Bimpli.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf CCP23S0002.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MH Titrerestaurant Memoiretechnique.pdf NOTICE_RGPD_ BIMPLI_Titres_Restaurants_validÃ©.pdf liste CNTR des affilies.xlsx",
       'Interaction_Server.20230314_142141_498.log.zip Logs_WDE.20230314_081105_645.log Logs_WDE.20230314_081105_645_1.log Stat_Server_URS.20230314_142315_903.log.zip URS.20230314_142408_610.log.zip',
       'HoT Biolite carbon credit prefinancing_v4.docx Kudura Advisory Proposal - Follow Ups.pptx unused',
       'Natixis_IF_Lux_I_Full Prospectus_Luxembourg_21022023 (2).pdf OpenFigiFunds_Bulk submission_Natixis template_Valeur_14.03.23_NIMI.xlsx'

for keyword in re.findall(r'\b\w+\b', regex):
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False, regex=False).astype(int)

# Nombre de mots clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count(regex)

# Ratio de mots clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots clés whitelistés
keywords_frequency = data['dlpfilename'].str.count(regex)
data['whitelist_keywords_frequency'] = data['dlpfilename'].str.count(regex)

import re

# Liste des mots-clés whitelistés
whitelist_keywords = [
    '0', 'Abonnement', 'Bulletin_Salaire', 'Vaccins', 'blob', 'booking', 'carte_tiers_payant', 'cerfa', 'curriculum',
    'devoir_gestion', 'image.jpg', 'image.png', 'impot', 'lettre_motivation', 'mission', 'payslip', 'permis', 'stage',
    'Attestation assurance', 'Bulletin notes', 'Bulletin_Salaire', 'CDI', 'CNI', 'CSE', 'CV', 'LIVRET FAMILLE', 'LM',
    'Passeport', 'bilan', 'professionnel', 'carte Bimpli', 'carte grise', 'entretiens', 'fiche de poste',
    'lettre de motivation', 'lettre de recommandation', 'lettre explicative', 'lettre hebdomadaire',
    'livret de famille', 'location', 'offres alternance', 'permis de construire', 'photo toilette',
    'poste assistant', 'resume', 'senior', 'stage', 'IMG', 'Image'
]

# Présence de mots-clés whitelistés
for keyword in whitelist_keywords:
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False).astype(int)

# Nombre de mots-clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count('|'.join(whitelist_keywords))

# Ratio de mots-clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots-clés whitelistés
data['whitelist_keywords_frequency'] = data['dlpfilename'].apply(lambda x: sum(x.count(keyword) for keyword in whitelist_keywords))

# Présence de mots-clés whitelistés ensemble
keyword_pairs = [
    ('bilan', 'professionnel'),
    ('carte', 'grise'),
    ('fiche', 'de', 'poste'),
    ('lettre', 'de', 'motivation'),
    ('lettre', 'de', 'recommandation'),
    ('lettre', 'explicative'),
    ('lettre', 'hebdomadaire'),
    ('livret', 'de', 'famille'),
    ('location'),
    ('offres', 'alternance'),
    ('permis', 'de', 'construire'),
    ('photo', 'toilette'),
    ('poste', 'assistant')
]

for keywords in keyword_pairs:
    feature_name = 'has_' + '_'.join(keywords)
    data[feature_name] = data['dlpfilename'].apply(lambda x: all(keyword in x for keyword in keywords)).astype(int)


from sklearn.ensemble import IsolationForest

# Entraînement du modèle Isolation Forest
clf = IsolationForest(random_state=42)
clf.fit(X_train_pca)

# Prédiction sur les données d'entraînement
y_train_pred = clf.predict(X_train_pca)

# Création d'un DataFrame avec les prédictions sur les données d'entraînement
df_train_predictions = pd.DataFrame({'Prediction': y_train_pred}, index=X_train.index)
df_train_predictions['Target'] = y_train  # Remplacez "y_train" par vos vraies étiquettes d'entraînement

# Compter les prédictions correctes sur les données d'entraînement
correct_train_predictions = df_train_predictions[df_train_predictions['Prediction'] == df_train_predictions['Target']]

# Compter les prédictions incorrectes sur les données d'entraînement
incorrect_train_predictions = df_train_predictions[df_train_predictions['Prediction'] != df_train_predictions['Target']]

# Compter les vrais positifs (VP) sur les données d'entraînement
vp_train_predictions = df_train_predictions[(df_train_predictions['Prediction'] == 1) & (df_train_predictions['Target'] == 1)]

# Afficher les résultats sur les données d'entraînement
print(f"Nombre total de prédictions sur les données d'entraînement : {len(df_train_predictions)}")
print(f"Prédictions correctes sur les données d'entraînement : {len(correct_train_predictions)}")
print(f"Prédictions incorrectes sur les données d'entraînement : {len(incorrect_train_predictions)}")
print(f"Nombre de VP prédits sur les données d'entraînement : {len(vp_train_predictions)}")

