for index, row in df.iterrows():
    if row['y_true'] == 1 and row['y_pred'] == 1:  # VP
        extensions = re.findall(r'\.[^.\\/:*?"<>|\r\n]+$', row['dlpfilename'])
        extensions_vp.update(extensions)
    elif row['y_true'] == 0 and row['y_pred'] == 1:  # FP
        extensions = re.findall(r'\.[^.\\/:*?"<>|\r\n]+$', row['dlpfilename'])
        extensions_fp.update(extensions)


'id', 'dhost', 'user', 'iup', 'date_alerte', 'dateFinContrat',
       'dlpdata', 'dlpmeta', 'raw_metier', 'dlpfilename', 'urlc', 'user_agent',
       'status', 'feature_contrat', 'feature_classification',
       'feature_department', 'feature_position_title', 'feature_download',
       'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3',
       'feature_downloadc4', 'feature_upload', 'feature_uploadc1',
       'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'
2022 - Frise enseignes chÃ¨que Bimpli CADO.pdf AE_BE-Lot1 - 20230315094857 - Signature 1.pdf AE_TR-Lot 2 - 20230315094909 - Signature 1.pdf Annexes au cadre de rÃ©ponse.zip Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BE BPU lot 1.xlsx BE DQE lot 1.xlsx CRT LOT 1 CADO.pdf CRT LOT 2 10 02.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf RÃ©fÃ©rences TITRES CADEAUX.pdf TR BPU lot 2.xlsx TR DQE lot 2.xlsx

import re

# Liste de toutes les extensions possibles dans le monde
extensions = ['.zip', '.log', '.7z', '.jpg', '.png', '.gif', '.pdf', '.docx', '.xlsx', '.pptx']

# Création de la colonne temp_ext avec les extensions présentes dans dlpfilename
df['temp_ext'] = df['dlpfilename'].apply(lambda x: re.findall(r'\.([^.\\/:*?"<>|\r\n]+)', x))

# Création de la feature is_only_specific_ext
df['is_only_specific_ext'] = df['temp_ext'].apply(lambda x: all(ext in extensions for ext in x) if x else False).astype(int)

# Suppression de la colonne temp_ext si nécessaire
df.drop('temp_ext', axis=1, inplace=True)


extensions = ['.zip', '.log', '.7z']  # Liste des extensions à rechercher

# Créer une colonne temporaire pour vérifier les extensions
df['temp_ext'] = df['dlpfilename'].str.lower().str.extract(r'(\.[a-zA-Z0-9]+)$')

# Vérifier si la colonne temporaire correspond exclusivement aux extensions spécifiées
df['is_only_specific_ext'] = (df['temp_ext'].isin(extensions) & ~df['temp_ext'].isna()).astype(int)

# Supprimer la colonne temporaire
df = df.drop(columns=['temp_ext'])


# Chargement des données
data = pd.read_csv("votre_fichier.csv")

# Feature Engineering

# Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

# Encodage des variables catégorielles
data = pd.get_dummies(data, columns=['feature_department', 'feature_position_title', 'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3', 'feature_downloadc4', 'feature_uploadc1', 'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'])

# Traitement des colonnes de texte supplémentaires
text_columns = ['iup', 'dlpdata', 'dlpmeta', 'raw_metier']
for column in text_columns:
    tfidf = TfidfVectorizer()
    tfidf_features = tfidf.fit_transform(data[column].astype(str))
    tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[column + '_' + c for c in tfidf.get_feature_names()])
    data = pd.concat([data, tfidf_df], axis=1)

 Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

import pandas as pd

# Création d'un DataFrame avec les données d'origine
data = pd.DataFrame({'raw_metier': ['NC', 'BPCE', 'MIROVA', 'NATIXIS BANK JSC', 'BPCE-IT', 'MIROVA UK', 'NATIXIS', 'USA', 'NFA',
                                   'NATIXIS INVESTMENT MANAGERS INTERNATIONAL', 'Ostrum', 'NATIXIS IM UK LIMITED',
                                   'NATIXIS LONDON BRANCH', 'NATIXIS NORTH AMERICA LLC', 'BPCE INFOGERANCE ET TECHNOLOGIES',
                                   'NATIXIS INVESTMENT MANAGERS', 'MIROVA US LLC', 'NATIXIS INTEREPARGNE', 'BPCE LEASE',
                                   'BIMPLI', 'NIE', 'NATIXIS IM, SUCURSAL EN ESPAÑA', 'NATIXIS - SUCURSAL EM PORTUGAL',
                                   'HOLDING PARIS', 'CIB_London', 'NLE', 'COMPAGNIE EUROPEENNE DE GARANTIES ET CAUTIONS SA',
                                   'BPCE FINANCEMENT', 'SEVENTURE PARTNERS', 'THEMATICS ASSET MANAGEMENT']})

# Regroupement des "raw metiers"
regroupements = {
    'MIROVA UK': 'MIROVA',
    'BPCE-IT': 'BPCE',
    'BPCE INFOGERANCE ET TECHNOLOGIES': 'BPCE',
    'BPCE LEASE': 'BPCE',
    'BPCE FINANCEMENT': 'BPCE',
    'NATIXIS BANK JSC': 'NATIXIS',
    'NATIXIS IM UK LIMITED': 'NATIXIS',
    'NATIXIS LONDON BRANCH': 'NATIXIS',
    'NATIXIS NORTH AMERICA LLC': 'NATIXIS',
    'NATIXIS INVESTMENT MANAGERS INTERNATIONAL': 'NATIXIS',
    'NATIXIS IM, SUCURSAL EN ESPAÑA': 'NATIXIS',
    'NATIXIS - SUCURSAL EM PORTUGAL': 'NATIXIS',
    'CIB_London': 'NATIXIS'
}

data['raw_metier'] = data['raw_metier'].apply(lambda x: regroupements.get(x, x))

# Regrouper les catégories restantes sous la catégorie "Other"
categories_autres = data['raw_metier'].unique()
categories_conservees = ['MIROVA', 'BPCE', 'NATIXIS']

data['raw_metier'] = data['raw_metier'].apply(lambda x: x if x in categories_conservees else 'Other')

# Répartition des "raw metiers" après les regroupements
repartition = data['raw_metier'].value_counts().reset_index()
repartition.columns = ['raw_metier', 'count']

print(repartition)

"2022 - Frise enseignes chÃ¨que Bimpli CADO.pdf AE_BE-Lot1 - 20230315094857 - Signature 1.pdf AE_TR-Lot 2 - 20230315094909 - Signature 1.pdf Annexes au cadre de rÃ©ponse.zip Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BE BPU lot 1.xlsx BE DQE lot 1.xlsx CRT LOT 1 CADO.pdf CRT LOT 2 10 02.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf RÃ©fÃ©rences TITRES CADEAUX.pdf TR BPU lot 2.xlsx TR DQE lot 2.xlsx",
       "AE Titres restaurant.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BIMPLI_Annexe - Bons Plans.pdf BIMPLI_Annexe_Liste-nationale-des-affilies-titres-restaurants.xlsx BIMPLI_Annexe_Ou commander en ligne avec sa carte resto.pdf BIMPLI_Annexe_Outils de commande Bimpli.pdf BIMPLI_Annexe_PrÃ©sentation Guest Club.pdf BIMPLI_Annexe_PrÃ©sentation dÃ©taillÃ©e de l'application Bimpli.pdf BIMPLI_Kit com nouveau client carte.zip BIMPLI_PrÃ©sentation des espaces personnel.pdf BPU-DQE.xlsx CCP - Annexe - RGPD.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MÃ©moire Technique_BIMPLI.pdf",
       "AE23S0002.pdf Annexe - Bons Plans Bimpli.pdf Annexe - Nos outils de commande.pdf Annexe - Ou commander en ligne.pdf Annexe - Plan_d'accompagnement_bÃ©nÃ©ficiaires.pdf Annexe - PrÃ©sentation Des Espaces personnels Bimpli.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf CCP23S0002.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MH Titrerestaurant Memoiretechnique.pdf NOTICE_RGPD_ BIMPLI_Titres_Restaurants_validÃ©.pdf liste CNTR des affilies.xlsx",
       'Interaction_Server.20230314_142141_498.log.zip Logs_WDE.20230314_081105_645.log Logs_WDE.20230314_081105_645_1.log Stat_Server_URS.20230314_142315_903.log.zip URS.20230314_142408_610.log.zip',
       'HoT Biolite carbon credit prefinancing_v4.docx Kudura Advisory Proposal - Follow Ups.pptx unused',
       'Natixis_IF_Lux_I_Full Prospectus_Luxembourg_21022023 (2).pdf OpenFigiFunds_Bulk submission_Natixis template_Valeur_14.03.23_NIMI.xlsx'

for keyword in re.findall(r'\b\w+\b', regex):
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False, regex=False).astype(int)

# Nombre de mots clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count(regex)

# Ratio de mots clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots clés whitelistés
keywords_frequency = data['dlpfilename'].str.count(regex)
data['whitelist_keywords_frequency'] = data['dlpfilename'].str.count(regex)

import re

# Liste des mots-clés whitelistés
whitelist_keywords = [
    '0', 'Abonnement', 'Bulletin_Salaire', 'Vaccins', 'blob', 'booking', 'carte_tiers_payant', 'cerfa', 'curriculum',
    'devoir_gestion', 'image.jpg', 'image.png', 'impot', 'lettre_motivation', 'mission', 'payslip', 'permis', 'stage',
    'Attestation assurance', 'Bulletin notes', 'Bulletin_Salaire', 'CDI', 'CNI', 'CSE', 'CV', 'LIVRET FAMILLE', 'LM',
    'Passeport', 'bilan', 'professionnel', 'carte Bimpli', 'carte grise', 'entretiens', 'fiche de poste',
    'lettre de motivation', 'lettre de recommandation', 'lettre explicative', 'lettre hebdomadaire',
    'livret de famille', 'location', 'offres alternance', 'permis de construire', 'photo toilette',
    'poste assistant', 'resume', 'senior', 'stage', 'IMG', 'Image'
]

# Présence de mots-clés whitelistés
for keyword in whitelist_keywords:
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False).astype(int)

# Nombre de mots-clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count('|'.join(whitelist_keywords))

# Ratio de mots-clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots-clés whitelistés
data['whitelist_keywords_frequency'] = data['dlpfilename'].apply(lambda x: sum(x.count(keyword) for keyword in whitelist_keywords))

# Présence de mots-clés whitelistés ensemble
keyword_pairs = [
    ('bilan', 'professionnel'),
    ('carte', 'grise'),
    ('fiche', 'de', 'poste'),
    ('lettre', 'de', 'motivation'),
    ('lettre', 'de', 'recommandation'),
    ('lettre', 'explicative'),
    ('lettre', 'hebdomadaire'),
    ('livret', 'de', 'famille'),
    ('location'),
    ('offres', 'alternance'),
    ('permis', 'de', 'construire'),
    ('photo', 'toilette'),
    ('poste', 'assistant')
]

for keywords in keyword_pairs:
    feature_name = 'has_' + '_'.join(keywords)
    data[feature_name] = data['dlpfilename'].apply(lambda x: all(keyword in x for keyword in keywords)).astype(int)


import re
import spacy

keywords = ['crédit', 'quarterlyreport', 'operations', 'hr', 'fees', 'comptes', 'comitã©', 'compte annuel',
            'contrôle', 'asset management', 'invest', 'roadmap', 'direction generale', 'portfolio', 'report',
            'process', 'project', 'compliance', 'habilitations', 'admin', 'cockpit', 'clientid', 'action_plan',
            'rã©fã©rentiel', 'keynote', 'strategic', 'allocated deduction', 'income', 'presentation', 'support',
            'draft', 'fund', 'valuations', 'procã©dure', 'managers', 'prod', 'sponsor', 'export', 'script', 'delete']

# Mapping français-anglais
mapping_fr_en = {
    'crédit': 'credit',
    'quarterlyreport': 'quarterly report',
    'operations': 'operations',
    'hr': 'hr',
    'fees': 'fees',
    'comptes': 'accounts',
    'comitã©': 'committee',
    'compte annuel': 'annual account',
    'contrôle': 'control',
    'asset management': 'asset management',
    'invest': 'invest',
    'roadmap': 'roadmap',
    'direction generale': 'general direction',
    'portfolio': 'portfolio',
    'report': 'report',
    'process': 'process',
    'project': 'project',
    'compliance': 'compliance',
    'habilitations': 'authorizations',
    'admin': 'admin',
    'cockpit': 'cockpit',
    'clientid': 'client id',
    'action_plan': 'action plan',
    'rã©fã©rentiel': 'repository',
    'keynote': 'keynote',
    'strategic': 'strategic',
    'allocated deduction': 'allocated deduction',
    'income': 'income',
    'presentation': 'presentation',
    'support': 'support',
    'draft': 'draft',
    'fund': 'fund',
    'valuations': 'valuations',
    'procã©dure': 'procedure',
    'managers': 'managers',
    'prod': 'prod',
    'sponsor': 'sponsor',
    'export': 'export',
    'script': 'script',
    'delete': 'delete'
}

# Initialiser le modèle spaCy
nlp = spacy.load('en_core_web_lg')

# Lemmatization des mots-clés et mapping
keywords_lemmatized = [mapping_fr_en.get(keyword, keyword) for keyword in keywords]

# Fonction de recherche des keywords dans dlpfilename
def find_keywords(text):
    doc = nlp(text)
    text_lemmatized = [token.lemma_.lower() for token in doc if token.is_alpha]
    for keyword in keywords_lemmatized:
        if keyword in text_lemmatized:
            return 1
    return 0

# Création de la colonne binaire pour la présence des keywords
df['contains_keywords'] = df['dlpfilename'].apply(find_keywords)


kw = ['crédit', 'quarterlyreport', 'operations', 'hr', 'fees', 
      'comptes', 'comitã©', 'compte annuel', 'contrôle', ' asset management ', 
      'invest',  'roadmap', 'direction generale', 'portfolio', 'report',
      'process', 'project','compliance','habilitations', 'admin', 'cockpit',
      'clientid', 'action_plan', 'rã©fã©rentiel', 'keynote', 'strategic', 'allocated deduction ',
      'income', 'presentation', 'support', 'draft', 'fund', 'valuations','procã©dure',  'managers',
      'prod', 'sponsor', 'export',  'script', 'delete']

from collections import Counter

# Filtrer les lignes VP et FP
df_filtered = df[df['target'].isin([0, 1])]

# Séparer les noms de fichiers VP et FP
vp_filenames = df_filtered[df_filtered['target'] == 1]['dlpfilename']
fp_filenames = df_filtered[df_filtered['target'] == 0]['dlpfilename']

# Concaténer les noms de fichiers VP
vp_filenames_concatenated = ' '.join(vp_filenames)

# Diviser en mots
vp_words = vp_filenames_concatenated.split()

# Calculer la fréquence des mots dans les VP
word_freq_vp = Counter(vp_words)

# Seuil de fréquence
threshold = 10

# Sélectionner les mots fréquents dans les VP
frequent_words_vp = [word for word, freq in word_freq_vp.items() if freq > threshold]

# Filtrer les mots fréquents des VP en éliminant ceux présents dans les FP
frequent_words_exclusive_vp = [word for word in frequent_words_vp if word not in ' '.join(fp_filenames).split()]

# Ajouter les mots fréquents exclusifs des VP aux mots-clés
keywords += frequent_words_exclusive_vp


