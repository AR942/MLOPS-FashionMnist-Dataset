Flask==2.2.2
imageio==2.25.0
keras==2.11.0
numpy==1.23.5
Pillow==9.4.0
python_resize_image==1.1.20
requests==2.28.1
tensorflow==2.11.0

'id', 'dhost', 'user', 'iup', 'date_alerte', 'dateFinContrat',
       'dlpdata', 'dlpmeta', 'raw_metier', 'dlpfilename', 'urlc', 'user_agent',
       'status', 'feature_contrat', 'feature_classification',
       'feature_department', 'feature_position_title', 'feature_download',
       'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3',
       'feature_downloadc4', 'feature_upload', 'feature_uploadc1',
       'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'
2022 - Frise enseignes chÃ¨que Bimpli CADO.pdf AE_BE-Lot1 - 20230315094857 - Signature 1.pdf AE_TR-Lot 2 - 20230315094909 - Signature 1.pdf Annexes au cadre de rÃ©ponse.zip Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BE BPU lot 1.xlsx BE DQE lot 1.xlsx CRT LOT 1 CADO.pdf CRT LOT 2 10 02.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf RÃ©fÃ©rences TITRES CADEAUX.pdf TR BPU lot 2.xlsx TR DQE lot 2.xlsx

import re

# Liste de toutes les extensions possibles dans le monde
extensions = ['.zip', '.log', '.7z', '.jpg', '.png', '.gif', '.pdf', '.docx', '.xlsx', '.pptx']

# Création de la colonne temp_ext avec les extensions présentes dans dlpfilename
df['temp_ext'] = df['dlpfilename'].apply(lambda x: re.findall(r'\.[^.\\/:*?"<>|\r\n]+$', x))

# Création de la feature is_only_specific_ext
df['is_only_specific_ext'] = df['temp_ext'].apply(lambda x: all(ext in extensions for ext in x) if x else False).astype(int)

# Suppression de la colonne temp_ext si nécessaire
df.drop('temp_ext', axis=1, inplace=True)


extensions = ['.zip', '.log', '.7z']  # Liste des extensions à rechercher

# Créer une colonne temporaire pour vérifier les extensions
df['temp_ext'] = df['dlpfilename'].str.lower().str.extract(r'(\.[a-zA-Z0-9]+)$')

# Vérifier si la colonne temporaire correspond exclusivement aux extensions spécifiées
df['is_only_specific_ext'] = (df['temp_ext'].isin(extensions) & ~df['temp_ext'].isna()).astype(int)

# Supprimer la colonne temporaire
df = df.drop(columns=['temp_ext'])


# Chargement des données
data = pd.read_csv("votre_fichier.csv")

# Feature Engineering

# Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

# Encodage des variables catégorielles
data = pd.get_dummies(data, columns=['feature_department', 'feature_position_title', 'feature_downloadc1', 'feature_downloadc2', 'feature_downloadc3', 'feature_downloadc4', 'feature_uploadc1', 'feature_uploadc2', 'feature_uploadc3', 'feature_uploadc4'])

# Traitement des colonnes de texte supplémentaires
text_columns = ['iup', 'dlpdata', 'dlpmeta', 'raw_metier']
for column in text_columns:
    tfidf = TfidfVectorizer()
    tfidf_features = tfidf.fit_transform(data[column].astype(str))
    tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[column + '_' + c for c in tfidf.get_feature_names()])
    data = pd.concat([data, tfidf_df], axis=1)

 Extraction des caractéristiques temporelles
data['alerte_fin_contrat_duration'] = (pd.to_datetime(data['date_alerte']) - pd.to_datetime(data['dateFinContrat'])).dt.days
data['alerte_month'] = pd.to_datetime(data['date_alerte']).dt.month
data['alerte_day_of_week'] = pd.to_datetime(data['date_alerte']).dt.dayofweek

# Traitement des colonnes texte
data['dhost_length'] = data['dhost'].apply(len)
data['user_agent_length'] = data['user_agent'].apply(len)
data['metier_keyword_count'] = data['raw_metier'].str.lower().str.count('votre_mot_clé')  # Remplacez 'votre_mot_clé' par le mot-clé pertinent
data['filename_length'] = data['dlpfilename'].apply(len)

# Caractéristiques basées sur les interactions entre les variables
data['classification_position'] = data['feature_classification'] + '_' + data['feature_position_title']
data['download_upload_ratio'] = data['feature_download'] / data['feature_upload']

import pandas as pd

# Création d'un DataFrame avec les données d'origine
data = pd.DataFrame({'raw_metier': ['NC', 'BPCE', 'MIROVA', 'NATIXIS BANK JSC', 'BPCE-IT', 'MIROVA UK', 'NATIXIS', 'USA', 'NFA',
                                   'NATIXIS INVESTMENT MANAGERS INTERNATIONAL', 'Ostrum', 'NATIXIS IM UK LIMITED',
                                   'NATIXIS LONDON BRANCH', 'NATIXIS NORTH AMERICA LLC', 'BPCE INFOGERANCE ET TECHNOLOGIES',
                                   'NATIXIS INVESTMENT MANAGERS', 'MIROVA US LLC', 'NATIXIS INTEREPARGNE', 'BPCE LEASE',
                                   'BIMPLI', 'NIE', 'NATIXIS IM, SUCURSAL EN ESPAÑA', 'NATIXIS - SUCURSAL EM PORTUGAL',
                                   'HOLDING PARIS', 'CIB_London', 'NLE', 'COMPAGNIE EUROPEENNE DE GARANTIES ET CAUTIONS SA',
                                   'BPCE FINANCEMENT', 'SEVENTURE PARTNERS', 'THEMATICS ASSET MANAGEMENT']})

# Regroupement des "raw metiers"
regroupements = {
    'MIROVA UK': 'MIROVA',
    'BPCE-IT': 'BPCE',
    'BPCE INFOGERANCE ET TECHNOLOGIES': 'BPCE',
    'BPCE LEASE': 'BPCE',
    'BPCE FINANCEMENT': 'BPCE',
    'NATIXIS BANK JSC': 'NATIXIS',
    'NATIXIS IM UK LIMITED': 'NATIXIS',
    'NATIXIS LONDON BRANCH': 'NATIXIS',
    'NATIXIS NORTH AMERICA LLC': 'NATIXIS',
    'NATIXIS INVESTMENT MANAGERS INTERNATIONAL': 'NATIXIS',
    'NATIXIS IM, SUCURSAL EN ESPAÑA': 'NATIXIS',
    'NATIXIS - SUCURSAL EM PORTUGAL': 'NATIXIS',
    'CIB_London': 'NATIXIS'
}

data['raw_metier'] = data['raw_metier'].apply(lambda x: regroupements.get(x, x))

# Regrouper les catégories restantes sous la catégorie "Other"
categories_autres = data['raw_metier'].unique()
categories_conservees = ['MIROVA', 'BPCE', 'NATIXIS']

data['raw_metier'] = data['raw_metier'].apply(lambda x: x if x in categories_conservees else 'Other')

# Répartition des "raw metiers" après les regroupements
repartition = data['raw_metier'].value_counts().reset_index()
repartition.columns = ['raw_metier', 'count']

print(repartition)

"2022 - Frise enseignes chÃ¨que Bimpli CADO.pdf AE_BE-Lot1 - 20230315094857 - Signature 1.pdf AE_TR-Lot 2 - 20230315094909 - Signature 1.pdf Annexes au cadre de rÃ©ponse.zip Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BE BPU lot 1.xlsx BE DQE lot 1.xlsx CRT LOT 1 CADO.pdf CRT LOT 2 10 02.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf RÃ©fÃ©rences TITRES CADEAUX.pdf TR BPU lot 2.xlsx TR DQE lot 2.xlsx",
       "AE Titres restaurant.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf BIMPLI_Annexe - Bons Plans.pdf BIMPLI_Annexe_Liste-nationale-des-affilies-titres-restaurants.xlsx BIMPLI_Annexe_Ou commander en ligne avec sa carte resto.pdf BIMPLI_Annexe_Outils de commande Bimpli.pdf BIMPLI_Annexe_PrÃ©sentation Guest Club.pdf BIMPLI_Annexe_PrÃ©sentation dÃ©taillÃ©e de l'application Bimpli.pdf BIMPLI_Kit com nouveau client carte.zip BIMPLI_PrÃ©sentation des espaces personnel.pdf BPU-DQE.xlsx CCP - Annexe - RGPD.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MÃ©moire Technique_BIMPLI.pdf",
       "AE23S0002.pdf Annexe - Bons Plans Bimpli.pdf Annexe - Nos outils de commande.pdf Annexe - Ou commander en ligne.pdf Annexe - Plan_d'accompagnement_bÃ©nÃ©ficiaires.pdf Annexe - PrÃ©sentation Des Espaces personnels Bimpli.pdf Annexes candidature.zip Attestation sur l'honneur_Outillage_Bimpli.pdf Attestation sur l'honneur_Outillage_Carte_Bimpli.pdf CCP23S0002.pdf DC1 BIMPLI.pdf DC2 BIMPLI.pdf MH Titrerestaurant Memoiretechnique.pdf NOTICE_RGPD_ BIMPLI_Titres_Restaurants_validÃ©.pdf liste CNTR des affilies.xlsx",
       'Interaction_Server.20230314_142141_498.log.zip Logs_WDE.20230314_081105_645.log Logs_WDE.20230314_081105_645_1.log Stat_Server_URS.20230314_142315_903.log.zip URS.20230314_142408_610.log.zip',
       'HoT Biolite carbon credit prefinancing_v4.docx Kudura Advisory Proposal - Follow Ups.pptx unused',
       'Natixis_IF_Lux_I_Full Prospectus_Luxembourg_21022023 (2).pdf OpenFigiFunds_Bulk submission_Natixis template_Valeur_14.03.23_NIMI.xlsx'

for keyword in re.findall(r'\b\w+\b', regex):
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False, regex=False).astype(int)

# Nombre de mots clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count(regex)

# Ratio de mots clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots clés whitelistés
keywords_frequency = data['dlpfilename'].str.count(regex)
data['whitelist_keywords_frequency'] = data['dlpfilename'].str.count(regex)

import re

# Liste des mots-clés whitelistés
whitelist_keywords = [
    '0', 'Abonnement', 'Bulletin_Salaire', 'Vaccins', 'blob', 'booking', 'carte_tiers_payant', 'cerfa', 'curriculum',
    'devoir_gestion', 'image.jpg', 'image.png', 'impot', 'lettre_motivation', 'mission', 'payslip', 'permis', 'stage',
    'Attestation assurance', 'Bulletin notes', 'Bulletin_Salaire', 'CDI', 'CNI', 'CSE', 'CV', 'LIVRET FAMILLE', 'LM',
    'Passeport', 'bilan', 'professionnel', 'carte Bimpli', 'carte grise', 'entretiens', 'fiche de poste',
    'lettre de motivation', 'lettre de recommandation', 'lettre explicative', 'lettre hebdomadaire',
    'livret de famille', 'location', 'offres alternance', 'permis de construire', 'photo toilette',
    'poste assistant', 'resume', 'senior', 'stage', 'IMG', 'Image'
]

# Présence de mots-clés whitelistés
for keyword in whitelist_keywords:
    data['has_' + keyword] = data['dlpfilename'].str.contains(keyword, case=False).astype(int)

# Nombre de mots-clés whitelistés
data['whitelist_keywords_count'] = data['dlpfilename'].str.count('|'.join(whitelist_keywords))

# Ratio de mots-clés whitelistés
data['whitelist_keywords_ratio'] = data['whitelist_keywords_count'] / data['dlpfilename'].str.len()

# Fréquence des mots-clés whitelistés
data['whitelist_keywords_frequency'] = data['dlpfilename'].apply(lambda x: sum(x.count(keyword) for keyword in whitelist_keywords))

# Présence de mots-clés whitelistés ensemble
keyword_pairs = [
    ('bilan', 'professionnel'),
    ('carte', 'grise'),
    ('fiche', 'de', 'poste'),
    ('lettre', 'de', 'motivation'),
    ('lettre', 'de', 'recommandation'),
    ('lettre', 'explicative'),
    ('lettre', 'hebdomadaire'),
    ('livret', 'de', 'famille'),
    ('location'),
    ('offres', 'alternance'),
    ('permis', 'de', 'construire'),
    ('photo', 'toilette'),
    ('poste', 'assistant')
]

for keywords in keyword_pairs:
    feature_name = 'has_' + '_'.join(keywords)
    data[feature_name] = data['dlpfilename'].apply(lambda x: all(keyword in x for keyword in keywords)).astype(int)


from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix

# Entraînement du modèle K-means
n_clusters = 2  # Nombre de clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans.fit(X_train)

# Prédiction sur les données non labélisées
y_pred = kmeans.predict(X_unlabeled)

# Calcul des informations demandées
confusion_mat = confusion_matrix(y_unlabeled, y_pred)  # Matrice de confusion
n_vp_predicted = confusion_mat[1, 1]  # Nombre de VP prédits
n_false_positives = confusion_mat[0, 1]  # Nombre de FP (non anomalies prédites comme anomalies)
n_false_negatives = confusion_mat[1, 0]  # Nombre de FN (anomalies prédites comme non anomalies)

# Calcul du pourcentage de VP prédits sur le nombre total de VP
total_vp = sum(y_unlabeled == 1)  # Nombre total de VP
percentage_vp_predicted = (n_vp_predicted / total_vp) * 100

# Affichage des résultats
print("Ce qu'il prédit en VP sur le nombre total de VP :", percentage_vp_predicted)
print("Ce qu'il prédit en VP mais qui sont en fait des FP :", n_false_positives)
print("Ce qu'il prédit en FP mais qui sont en fait des VP :", n_false_negatives)



